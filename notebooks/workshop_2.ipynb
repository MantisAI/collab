{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop: App Review Analysis with Few-Shot Learning\n",
    "\n",
    "In this workshop, we'll explore how to acquire, clean, and analyze app reviews using various techniques, including Few-Shot Learning with Large Language Models (LLMs).\n",
    "\n",
    "## Objectives:\n",
    "1. Acquire and analyze reviews for your own app\n",
    "2. Explore a benchmark dataset of app reviews\n",
    "3. Clean and preprocess review data\n",
    "4. Build baseline classifiers for sentiment analysis\n",
    "5. Apply Few-Shot Learning techniques using LLMs\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition\n",
    "\n",
    "### Step 1: Acquire reviews for your own app\n",
    "\n",
    "We'll use the Google Play Scraper library to collect reviews for your app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_play_scraper import app, reviews\n",
    "import pandas as pd\n",
    "\n",
    "# Replace 'your.app.id' with your actual app ID\n",
    "app_id = 'com.whatsapp'\n",
    "app_reviews, _ = reviews(app_id, count=1000)  # Fetching 1000 reviews\n",
    "\n",
    "# Convert to DataFrame\n",
    "app_reviews_df = pd.DataFrame(app_reviews)\n",
    "print(app_reviews_df.head())\n",
    "print(f\"Total reviews collected: {len(app_reviews_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Hugging Face \"google-play-review\" dataset\n",
    "\n",
    "We'll use this dataset as a benchmark for positive/negative review classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load Google Play Review dataset from Hugging Face\n",
    "hf_reviews = load_dataset(\"jakartaresearch/google-play-review\")\n",
    "hf_reviews_df = hf_reviews['train'].to_pandas()\n",
    "print(hf_reviews_df.head())\n",
    "print(f\"Total reviews in the dataset: {len(hf_reviews_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Analysis and Cleaning\n",
    "\n",
    "### Step 3: Clean the datasets\n",
    "\n",
    "We'll filter languages, remove short reviews, and handle unbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langdetect\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return langdetect.detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "def clean_dataset(df, text_column, min_words=3, target_languages=['en']):\n",
    "    # Detect language\n",
    "    df.loc[:, 'detected_language'] = df.loc[:, text_column].apply(detect_language)\n",
    "    \n",
    "    # Filter by language\n",
    "    df = df[df.loc[:, 'detected_language'].isin(target_languages)]\n",
    "    \n",
    "    # Remove short reviews\n",
    "    df.loc[:, 'word_count'] = df.loc[:, text_column].apply(lambda x: len(str(x).split()))\n",
    "    df = df[df.loc[:, 'word_count'] >= min_words]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean your app reviews\n",
    "clean_app_reviews = clean_dataset(app_reviews_df, 'content')\n",
    "#rename score into stars\n",
    "clean_app_reviews = clean_app_reviews.rename(columns={'score':'stars'})\n",
    "print(\"Your app reviews after cleaning:\")\n",
    "print(clean_app_reviews.head())\n",
    "print(f\"Reviews remaining: {len(clean_app_reviews)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Hugging Face dataset\n",
    "clean_hf_reviews = clean_dataset(hf_reviews_df, 'text')\n",
    "print(\"\\nHugging Face reviews after cleaning:\")\n",
    "print(clean_hf_reviews.head())\n",
    "print(f\"Reviews remaining: {len(clean_hf_reviews)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance in Hugging Face dataset\n",
    "print(\"\\nClass balance in Hugging Face dataset:\")\n",
    "print(clean_hf_reviews['stars'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "hf_counts = clean_hf_reviews['stars'].value_counts(normalize=True).sort_index()\n",
    "app_counts = clean_app_reviews['stars'].value_counts(normalize=True).sort_index()\n",
    "\n",
    "indices = np.arange(len(hf_counts))\n",
    "\n",
    "bar_width = 0.35\n",
    "\n",
    "ax.bar(indices, hf_counts, width=bar_width, label=\"Hugging Face dataset\")\n",
    "ax.bar(indices + bar_width, app_counts, width=bar_width, label=\"Your app reviews\")\n",
    "\n",
    "ax.set_title(\"Class Balance Comparison: Hugging Face vs Your App Reviews\")\n",
    "ax.set_xlabel(\"Rating\")\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "\n",
    "ax.set_xticks(indices + bar_width / 2)\n",
    "ax.set_xticklabels(hf_counts.index)\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "### Step 4: Create categorical variables and clean textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Preprocess text in both datasets\n",
    "clean_app_reviews['processed_text'] = clean_app_reviews['content'].apply(preprocess_text)\n",
    "clean_hf_reviews['processed_text'] = clean_hf_reviews['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bag-of-words features for Hugging Face dataset\n",
    "vectorizer = CountVectorizer(max_features=1000, stop_words='english',\n",
    "                             min_df=5\n",
    "                             )\n",
    "bow_features = vectorizer.fit(np.concatenate((clean_hf_reviews['processed_text'], clean_app_reviews['processed_text'])))\n",
    "bow_features_app = vectorizer.transform(clean_app_reviews['processed_text'])\n",
    "bow_features_hf = vectorizer.transform(clean_hf_reviews['processed_text'])\n",
    "\n",
    "print(\"Bag-of-words features shape:\", bow_features_app.shape)\n",
    "print(\"Bag-of-words features shape HF dataset:\", bow_features_hf.shape)\n",
    "print(\"\\nSample processed text:\")\n",
    "print(clean_hf_reviews['processed_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "\n",
    "# Get the feature names (words) from the vectorizer\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Function to generate word clouds for a dataset grouped by star ratings and display them side by side\n",
    "def generate_wordcloud_side_by_side(bow_features_hf, bow_features_app, reviews_hf_df, reviews_app_df, rating_column, dataset_hf_name, dataset_app_name):\n",
    "    # Reset the indices to ensure alignment between the dataframe and bow_features\n",
    "    reviews_hf_df = reviews_hf_df.reset_index(drop=True)\n",
    "    reviews_app_df = reviews_app_df.reset_index(drop=True)\n",
    "    \n",
    "    # Get unique rating values for alignment\n",
    "    ratings = sorted(reviews_hf_df[rating_column].unique())\n",
    "    \n",
    "    # Create subplots, one row per rating\n",
    "    fig, axes = plt.subplots(len(ratings), 2, figsize=(15, 5 * len(ratings)))  # Adjust height based on the number of ratings\n",
    "    \n",
    "    for i, rating_value in enumerate(ratings):\n",
    "        # Hugging Face Dataset - Get the reviews for this rating\n",
    "        hf_indices = reviews_hf_df[reviews_hf_df[rating_column] == rating_value].index.tolist()\n",
    "        hf_bow = bow_features_hf[hf_indices].sum(axis=0).A1\n",
    "        hf_word_freq = dict(zip(feature_names, hf_bow))\n",
    "\n",
    "        # Generate word cloud for Hugging Face dataset\n",
    "        wordcloud_hf = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(hf_word_freq)\n",
    "        \n",
    "        # Plot the Hugging Face word cloud\n",
    "        axes[i, 0].imshow(wordcloud_hf, interpolation='bilinear')\n",
    "        axes[i, 0].set_title(f'{rating_value} Stars in {dataset_hf_name}')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # App Reviews Dataset - Get the reviews for this rating\n",
    "        app_indices = reviews_app_df[reviews_app_df[rating_column] == rating_value].index.tolist()\n",
    "        app_bow = bow_features_app[app_indices].sum(axis=0).A1\n",
    "        app_word_freq = dict(zip(feature_names, app_bow))\n",
    "\n",
    "        # Generate word cloud for App Reviews dataset\n",
    "        wordcloud_app = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(app_word_freq)\n",
    "        \n",
    "        # Plot the App Reviews word cloud\n",
    "        axes[i, 1].imshow(wordcloud_app, interpolation='bilinear')\n",
    "        axes[i, 1].set_title(f'{rating_value} Stars in {dataset_app_name}')\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate word clouds side by side for Hugging Face dataset and App Reviews dataset\n",
    "generate_wordcloud_side_by_side(bow_features_hf, bow_features_app, clean_hf_reviews, clean_app_reviews, 'stars', 'Hugging Face Dataset', 'Your App Reviews Dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do two things:\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Download required NLTK data if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Custom stopwords - extend the default list to remove 'whatsapp' and 'app'\n",
    "stop_words = list(set(stopwords.words('english')) - set(['whatsapp', 'app']))\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to clean and lemmatize text\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphabetic characters and lowercase the text\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    \n",
    "    # Tokenize the text, remove stopwords, and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "    \n",
    "    # Join tokens back into a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the datasets\n",
    "clean_hf_reviews['processed_text'] = clean_hf_reviews['processed_text'].apply(preprocess_text)\n",
    "clean_app_reviews['processed_text'] = clean_app_reviews['processed_text'].apply(preprocess_text)\n",
    "\n",
    "# Vectorization after preprocessing\n",
    "vectorizer = CountVectorizer(max_features=1000, stop_words=list(stop_words), min_df=5)\n",
    "bow_features = vectorizer.fit(np.concatenate((clean_hf_reviews['processed_text'], clean_app_reviews['processed_text'])))\n",
    "bow_features_app = vectorizer.transform(clean_app_reviews['processed_text'])\n",
    "bow_features_hf = vectorizer.transform(clean_hf_reviews['processed_text'])\n",
    "\n",
    "# Print the shapes of the resulting bag-of-words features\n",
    "print(\"Bag-of-words features shape:\", bow_features_app.shape)\n",
    "print(\"Bag-of-words features shape HF dataset:\", bow_features_hf.shape)\n",
    "print(\"\\nSample processed text:\")\n",
    "print(clean_hf_reviews['processed_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud_side_by_side(bow_features_hf, bow_features_app, clean_hf_reviews, clean_app_reviews, 'stars', 'Hugging Face Dataset', 'Your App Reviews Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Creation\n",
    "\n",
    "### Step 5: Create a baseline classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Classifier\n",
    "\n",
    "### Steps:\n",
    "1. **Extract the top 50 unigrams, bigrams, and trigrams** for each star rating.\n",
    "2. **Assign weights** to each n-gram based on its frequency and relevance to each rating.\n",
    "3. If a review does not contain any of the top n-grams, we will assign a **default rating of 3** (neutral).\n",
    "\n",
    "### Why This is a Good First Baseline:\n",
    "1. **Simplicity**: \n",
    "   - This baseline is simple and interpretable, making it an excellent teaching tool for students to understand basic classification techniques.\n",
    "   \n",
    "2. **Real-World Relevance**:\n",
    "   - The use of **n-grams** mimics a common practice in Natural Language Processing (NLP), where short sequences of words often carry meaning in sentiment analysis.\n",
    "   \n",
    "3. **Performance Benchmark**:\n",
    "   - This baseline classifier will likely have modest performance, which sets the stage for more complex models (e.g., logistic regression, neural networks) to improve upon.\n",
    "   - It helps students understand the importance of **starting simple** and using baselines to track progress in model development.\n",
    "\n",
    "4. **Weighted by Frequency**:\n",
    "   - By assigning weights based on **frequency statistics**, students can see how **statistical approaches** can offer a quick, lightweight solution before diving into more computationally expensive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Step 1: Extract the top 10 unigrams, bigrams, and trigrams for each star rating\n",
    "\n",
    "# Initialize the CountVectorizer to extract unigrams, bigrams, and trigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), max_features=1000, stop_words='english')\n",
    "\n",
    "# Fit the vectorizer on the entire dataset\n",
    "vectorizer.fit(clean_hf_reviews['processed_text'])\n",
    "\n",
    "# Create bag-of-words features\n",
    "bow_features = vectorizer.transform(clean_hf_reviews['processed_text'])\n",
    "\n",
    "# Convert the bag-of-words matrix to a dataframe for easier manipulation\n",
    "bow_df = pd.DataFrame(bow_features.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Add the 'stars' column to the dataframe\n",
    "bow_df['stars'] = clean_hf_reviews['stars'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: For each rating, get the top 50 n-grams\n",
    "n = 50\n",
    "top_ngrams_by_rating = defaultdict(list)\n",
    "for rating in sorted(bow_df['stars'].unique()):\n",
    "    # Filter reviews by rating\n",
    "    rating_reviews = bow_df[bow_df['stars'] == rating]\n",
    "    \n",
    "    # Sum up the n-gram counts for all reviews with this rating\n",
    "    rating_ngram_counts = rating_reviews.drop('stars', axis=1).sum()\n",
    "    \n",
    "    # Get the top 10 n-grams for this rating\n",
    "    top_ngrams = rating_ngram_counts.nlargest(n)\n",
    "    \n",
    "    # Store the top n-grams in the dictionary\n",
    "    top_ngrams_by_rating[rating] = top_ngrams.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Assign weights based on statistical frequency\n",
    "ngram_weights = {}\n",
    "for rating, ngrams in top_ngrams_by_rating.items():\n",
    "    for ngram in ngrams:\n",
    "        # Frequency-based weight: the more common the n-gram for a rating, the higher its weight\n",
    "        ngram_weights[ngram] = rating  # Assign the rating as the weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create the baseline classifier\n",
    "def baseline_classifier(review):\n",
    "    # Preprocess the review\n",
    "    processed_review = preprocess_text(review)\n",
    "    \n",
    "    # Check for the presence of n-grams and sum their weights\n",
    "    total_weight = 0\n",
    "    for ngram, weight in ngram_weights.items():\n",
    "        if ngram in processed_review:\n",
    "            total_weight += weight\n",
    "    \n",
    "    # If the review contains matching n-grams, predict based on the weighted sum\n",
    "    if total_weight > 0:\n",
    "        return round(total_weight / len(processed_review.split()))  # Average the weights\n",
    "    else:\n",
    "        # If no n-grams match, return a neutral rating of 3\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the baseline classifier to a test dataset\n",
    "clean_hf_reviews['predicted_rating'] = clean_hf_reviews['processed_text'].apply(baseline_classifier)\n",
    "\n",
    "# Evaluate the baseline classifier (Optional)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(clean_hf_reviews['stars'], clean_hf_reviews['predicted_rating'])\n",
    "print(f'Baseline classifier accuracy: {accuracy* 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Prepare data\n",
    "X = bow_features_hf\n",
    "y = clean_hf_reviews['stars']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Naive Classifier\n",
    "\n",
    "### Step 6: Train and evaluate a naive classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train Logistic Regression classifier\n",
    "lr_classifier = LogisticRegression(random_state=42)\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_lr = lr_classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Few-Shot Learning and LLM\n",
    "\n",
    "### Step 7: Apply Few-Shot Learning using an LLM\n",
    "\n",
    "We'll use a pre-trained model for sentiment analysis as an example of Few-Shot Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Use a pre-trained model for text classification\n",
    "classifier = pipeline('text-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review(review_text):\n",
    "    # Use the classifier to predict sentiment for each review\n",
    "    score = classifier(review_text)[0]['score']\n",
    "\n",
    "    # Convert the score to a star rating (1 to 5)\n",
    "    # if 0.0 <= score < 0.2: 1, if 0.2 <= score < 0.4: 2, and so on\n",
    "    predicted_rating = int(1 + round(score * 4))\n",
    "    return predicted_rating\n",
    "\n",
    "# Apply the classifier to all reviews in clean_hf_reviews\n",
    "clean_hf_reviews['predicted_rating'] = clean_hf_reviews['processed_text'].apply(classify_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing the Results\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(clean_hf_reviews['stars'], clean_hf_reviews['predicted_rating'])\n",
    "print(f\"Accuracy: {accuracy*100:.2f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(classification_report(clean_hf_reviews['stars'], clean_hf_reviews['predicted_rating']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this workshop, we've covered:\n",
    "\n",
    "1. Acquiring and cleaning app review data\n",
    "2. Exploring a benchmark dataset\n",
    "3. Building baseline classifiers for sentiment analysis\n",
    "4. Applying Few-Shot Learning techniques using pre-trained LLMs\n",
    "\n",
    "These techniques can be applied to various natural language processing tasks, especially when dealing with limited labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
